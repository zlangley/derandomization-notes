\begin{lecture}{1}{The Class prBPP}{January 23, 2020}
\label{lec:01}

\subsection*{The Upshot}

\begin{enumerate}
  \item Randomness is necessary in many computational settings. Is randomness
    necessary for polynomial-time computation? There are good reasons to expect
    it is not! This is a fundamental question we will investigate in this
    course.
  \item Promise problems are a natural generalization of languages, and the
    theory of probabilistic algorithms for promise problems is richer than
    for that of languages.
  \item The class $\prBPP$ has a complete problem ($\CAPP$) under deterministic
    polynomial-time reductions. Hence to prove $\prBPP = \prP$ it suffices
    to give a deterministic polynomial-time algorithm for $\CAPP$.
  \item The class $\prBPP$ admits a time-hierarchy theorem.
  \item Probabilistic search problems can be reduced to probabilistic decision
    problems, meaning the theory we will develop around the class of decision
    problems $\prBPP$ will apply to the search analog as well.
\end{enumerate}

\subsection{Promise Problems and prBPP}

A \emph{promise problem} is a pair $\Pi = (\Pi_Y, \Pi_N) \subseteq \{0, 1\}^*
\times \{0, 1\}^*$ such that $\Pi_Y \cap \Pi_N = \emptyset$. The set $\Pi_Y$
captures all of the ``yes'' instances, and the set $\Pi_N$ captures all of the
``no'' instances. The set $\Pi_Y \cup \Pi_N$ is called the \emph{promise}. We
say that a Turing machine $M$ \emph{solves} a promise problem $\Pi$ if on
input $x \in \Pi_Y$ it accepts and on input $x \in \Pi_N$ it rejects. Crucially,
for inputs not in the promise, the machine can behave arbitrarily.

Promise problems generalize decision problems (languages) in the case that
$\Pi_Y \cup \Pi_N = \{0,1\}^*$. Complexity classes based on decision problems
have obvious analogs based on promise problems. For example, $\prP$ is the
class of all promise problems solvable by a deterministic polynomial-time
Turing machine. Similarly, the class $\prBPP$ is the class of promise problems
solvable by a probabilistic polynomial-time Turing machine with error
probability at most 1/3 for all instances. The class $\prBPP$ will be one of
our main objects of study, so its definition is worth repeating carefully.

\begin{definition}[$\prBPP$]
  A promise problem $\Pi = (\Pi_Y, \Pi_N)$ is in $\prBPP$ if and only if there
  exists a polynomial $p(n)$ and a probabilistic Turing machine $M$ such that
  \begin{enumerate}
    \item if $x \in \Pi_Y$, then $M$ accepts input $x$ with probability greater
      than $2/3$ after $p(n)$ steps, and
    \item if $x \in \Pi_N$, then $M$ rejects input $x$ with probability greater
      than $2/3$ after $p(n)$ steps.
  \end{enumerate}
\end{definition}

The constant $2/3$ in the definition of $\prBPP$ is not important; it
can be replaced by any other constant greater than 1/2, which we will discuss
in more detail in the next section.%

\begin{proposition}
  If $\prBPP = \prP$, then $\BPP = \P$.
\end{proposition}

\begin{proof}
  If $\Pi \in \BPP$, then trivially $\Pi \in \prBPP$, and therefore by
  assumption, $\Pi \in \prP$. As $\Pi$ is a decision problem, it follows
  that $\Pi \in \P$.
\end{proof}

What do we know about the placement $\BPP$ with respect to other complexity
classes? An easy inclusion is $\BPP \subseteq \PSPACE$.

\begin{proposition}
  $\BPP \subseteq \PSPACE$.
\end{proposition}
\begin{proof}
  In one sentence: Simulate the $\BPP$ algorithm with all possible random
  strings.

  In more detail, let $L \in \BPP$ and let $M$ be a probabilistic Turing
  machine that solves $L$ in polynomial time $p(n)$. We can design a
  deterministic machine $M'$ that solves $L$ as follows. On input $x$, $M'$
  simulates $M$ on input $(x, r)$ for every $r \in \set{0,1}^{p(n)}$ and counts
  the number of times $M$ accepts. If $M$ accepts for more than $\frac{2}{3}
  \cdot 2^{p(n)}$ choices of $r$, then $M'$ accepts. Otherwise, $M'$ rejects.

  Clearly $M'$ accepts $L$ if and only if $M$ accepts $L$. The space
  requirements of $M'$ are the space requirements of $M$ plus the $p(n)$ bits
  for $r$ plus the $p(n)$ bits for the counter, and hence $M$ decides $L$ in
  polynomial space.
\end{proof}

Of course we cannot yet prove $\BPP \ne \PSPACE$ because we cannot yet prove
$\P \ne \PSPACE$. The deterministic time hierarchy theorem tells us that $P \ne
\EXP$, but suprisingly, it is still only a conjecture that $\BPP \ne \NEXP$!

\subsection{Equivalent Definitions of prBPP via Amplification}

Let $\prBPP[\delta(n)]$ be the class of promise problems that are solved by a
polynomial-time Turing machine with error probability at most $\delta(n)$ on
all input strings of length $n$. Notice that $\prBPP = \prBPP[1/3]$. We have
already mentioned that for all constant $\delta_0 > 1/2$, $\prBPP[\delta_0] =
\prBPP$. However, perhaps surprisingly, the class $\prBPP[1/2]$ appears much
more powerful than $\prBPP$: for example, $\NP \subseteq \prBPP[1/2]$.%
\footnote{The non-promise class corresponding to what we call $\prBPP[1/2]$ is
called $\PP$.}

Recall the following additive Chernoff bound.

\begin{theorem}[Additive Chernoff lower tail bound]\label{thm:chernoff}
  Let $X_1, \dots, X_n$ are independent random variables where $X_i \sim
  \Bern(p)$, let $X := \sum_{i=1}^n X_i$, and let $\mu = \Exp X$. For all
  $\lambda \ge 0$, \[
    \Pr\paren*{X \le \mu - \lambda} \le \exp\paren*{-\frac{\lambda^2}{2\mu}}.
  \]
\end{theorem}

\begin{proposition}[Majority trick]\label{prop:maj}
  Let $\eps \in (0, \frac{1}{2}]$, let $p \ge \frac{1}{2} + \eps$, and let $X_1,
  \dots, X_t \sim \Bern(p)$ be i.i.d. The majority of $X_1, \dots, X_t$ is 1
  with probability at least $1 - \exp(-\eps^2 t/2)$.
\end{proposition}

\begin{proof}
  Let $X := \sum_{i=1}^t X_i$ and let $\mu = \Exp X$ and observe that $\mu \ge
  t/2 + t\eps$ and $\mu \le t$. If $X \le t/2$, then the majority is 0. By the
  Chernoff bound of Theorem~\ref{thm:chernoff}, the probability of this bad
  event is \[
    \Pr(X \le t/2) = \Pr(X \le \mu - \eps t) < \exp\paren*{-\frac{\eps^2t^2}{2\mu}} \le \exp\paren*{-\frac{\eps^2t}{2}}.
  \]
  Hence the output is 1 with probability at most $1 - \exp(-\eps^2 t/2)$.
\end{proof}

\begin{corollary}\label{cor:bpp-small-error}
  If $\delta(n)$ satisfies $\delta(n) = 2^{-\poly(n)}$ and $1/2 - \delta =
  1/\poly(n)$, then $\prBPP[\delta(n)] = \prBPP$.
\end{corollary}

\begin{proof}
  By Proposition~\ref{prop:maj}, any algorithm with success rate $\delta_0 =
  1/2 + \eps$ can be used to produce an algorithm with success rate $\delta_1 >
  \delta_0$ by repeating the algorithm independently $t :=
  2\ln(\delta_1)/\eps^2$ times and taking the majority. The conditions on
  $\delta_0$ and $\delta_1$ ensure that $t = \poly(n)$.
\end{proof}

\comment{TODO: reference Lecture 5, in which the theorem below is proved}

\begin{definition}
  An \emph{$(\eps, \delta)$-sampler} is a function $s :
  \set{0,1}^{\overline{T}} \times \set{0,1}^\ell \to \set{0,1}^T$ such that for
  all $f : \set{0,1}^T \to \set{0,1}$ with probability at least $1 - \delta$ 
  over $z \in \set{0,1}^{\overline{T}}$ it holds that \[
    |\Exp_i f(s(z, i)) - \Exp_x f(x)| \le \eps.
  \]
\end{definition}

\begin{theorem}
  For all $\eps > 0$ there exists a polynomial-time computable $(\eps,
  \delta)$-sampler with $\overline{T} = \poly(T)$, $\ell = O(\log
  \overline{T}/\eps)$ and $\delta = 2^{\overline{T}^\eps - \overline{T}}$.
\end{theorem}

\begin{corollary}\label{cor:bpp-small-random}
  For all $\eps > 0$, the class $\prBPP$ can be equivalently defined with an
  error of $2^{T^\eps - T}$ instead of 1/3, where $T = T(n)$ is the number of
  random bits.

  In other words, $2^{T^\eps}$ out of the $2^T$ random bit-strings are exceptional.
\end{corollary}

\begin{corollary}
  $\BPP \subseteq \Ppoly$.
\end{corollary}

\begin{proof}
    Let $L \in \BPP$. By Corollary~\ref{cor:bpp-small-error}, there exists a
    probabilistic polynomial-time machine $M$ solving $L$ with error
    probability at most $2^{-2n}$.

    Fix $n$ and let $f_n : \{0, 1\}^n \to \{0, 1\}$ be the indicator for $L
    \cap \{0, 1\}^n$. Let $A_x$ be the event that $M(x, r) \ne f_n(x)$, where
    the randomness is over the choice of $r$. For any fixed $x \in \{0, 1\}^n$,
    we have
    \begin{align*}
      \Pr\brac*{A_x} \leq 2^{-2n}.
    \end{align*}
    Union bounding over all inputs of length $n$, \[
      \Pr\brac*{\bigcup_{x \in \set{0, 1}^n} A_x}
      \leq \sum_{x \in \set{0,1}^n} \Pr_r\brac*{A_x}
      \leq \sum_{x \in \set{0,1}^n} 2^{-2n}
      < 1.
    \]
    There is thus some $r^*$ such that $A_x$ does not occur for any $x$. In
    other words, $M(x,r^*) = f_n(x)$ for all inputs $x$ of length $n$. Hence the
    machine that on input $x$ and advice $r^*$ simulates $M$ on input $(x,
    r^*)$ solves $L$ in polynomial time using polynomial bits of advice.
\end{proof}


\subsection{The Circuit Acceptance Probability Problem (CAPP)}

The \emph{acceptance probability} of a circuit is the fraction of inputs for
which it outputs 1.

\begin{definition}[$\CAPP$]
  $\CAPP$ is the promise problem where
  \begin{enumerate}
    \item $\CAPP_Y$ is the set of all descriptions of circuits that have
      acceptance probability at least 2/3, and
    \item $\CAPP_N$ is the set of all descriptions of circuits that have
      acceptance probability at most 1/3.
  \end{enumerate}
\end{definition}

\begin{theorem}[$\CAPP$ is $\prBPP$-complete]\label{thm:complete}
  $\CAPP$ is complete for $\prBPP$ under deterministic polynomial-time reductions.
\end{theorem}

Recall from the reduction in Cook's theorem that every Turing machine can be
efficiently converted to an equivalent circuit for a fixed input length.
\begin{lemma}\label{lem:tm-to-circuit}
  For all $n \in \N$ there is an $O(t^2)$-time algorithm that, on input
  $\langle M \rangle$ for some Turing machine $M$, outputs a circuit $C_n$ such
  that $C_n$ accepts $x \in \{0, 1\}^n$ if and only if $x \in L(M)$.
\end{lemma}

\begin{proof}[Proof of Theorem~\ref{thm:complete}]
  The problem $\CAPP$ is easily seen to be in $\prBPP$: consider the algorithm
  that simply simulates the input circuit on a randomly chosen input. So let us
  show that every problem in $\prBPP$ can be reduced to $\CAPP$ in
  deterministic polynomial time. Let $\Pi \in \prBPP$ and let $M$ be the
  probabilistic polynomial-time Turing machine solving $\Pi$. Given an input
  $x$, by Lemma~\ref{lem:tm-to-circuit}, we can construct in polynomial-time a
  circuit $C_x$ such that $C_x(r) = M(x, r)$ for all $r$. Hence if $x \in
  \Pi_Y$, then $C_x \in \CAPP_Y$, and if $x \in \Pi_N$, then $C_x \in \CAPP_N$.
\end{proof}

\subsection{A Time-Hierarchy Theorem}

\begin{theorem}
  $\prBPTIME[T \log{T}] \subsetneq \prBPTIME[T]$
\end{theorem}

\comment{TODO: Why standard diag fails for BPP. Why it fails for prBPP. 
         Why lazy diag works for prBPP. Why it still fails for BPP.}

\subsection{Reducing Search Problems to Decision Problems}

\subsection{Two-Sided Error vs.\ One-Sided Error}

\begin{theorem}\label{thm:bpp-subset-rprp}
    $\prBPP \subseteq \prRP^\prRP$.
\end{theorem}

\begin{proofsk}
    Let $\Gamma = (\Gamma_Y, \Gamma_N)$ be the following promise problem:
    \begin{itemize}
        \item $\Gamma_Y = \set{\langle C \rangle \mid \Pr\brac{C(r) = 1} = 1}$,
        \item $\Gamma_N = \set{\langle C \rangle \mid \Pr\brac{C(r) = 0} \geq 1 - 2^{\sqrt{|r|} - |r|}}$.
    \end{itemize}
    One can verify that $\Gamma$ is indeed in $\prRP$.

    Let $\Pi = (\Pi_Y, \Pi_N) \in \prBPP$.
    By Corollary~\ref{cor:bpp-small-random}, there is a probabilistic polynomial time 
    Turing Machine $M$ deciding $x \in \Pi_Y$ or $x \in \Pi_N$ with $2^{\sqrt{p(|x|)} 
    - p(|x|)}$ error probability over $p(|x|)$ random bits.
    
    Consider the following $\prRP^\Gamma$ algorithm that, given $x \in \Pi_Y \cup \Pi_N$, 
    decides if $x \in \Pi_Y$ or $x \in \Pi_N$.
    \begin{itemize}
        \item Compile $M(x,r)$ to a circuit $C_x(r)$ which accepts or rejects at most 
            $2^{\sqrt{|r|}}$ of its inputs (see Lemma~\ref{lem:tm-to-circuit}, 
            Theorem~\ref{thm:complete}).
        \item Fixes the first $|r|/2$ inputs of $C_x(r)$ randomly, yielding the circuit 
            $C'_x(r')$.
        \item Queries the $\Gamma$ oracle with a description of $C'_x(r')$ and returns 
            the same answer.
    \end{itemize}

    \comment{TODO: Finish up and turn into proper proof. 
    Show $\Pi_Y$ gets $1$ whp, and $\Pi_N$ gets $0$ all the time.}
\end{proofsk}

\begin{corollary}
    $\prBPP = \prP \iff \prRP = \prP$.
\end{corollary}

\begin{proof}
    If $\prBPP = \prP$, then it is clear that $\prRP = \prP$ since $\prP \subseteq
    \prRP \subseteq \prBPP$.

    Conversely, if $\prRP = \prP$, then by Theorem~\ref{thm:bpp-subset-rprp},
    $\prBPP \subseteq \prRP^\prRP = \prP^\prP = \prP$.
    Since $\prP \subseteq \prBPP$, we have $\prBPP = \prP$.
\end{proof}

\end{lecture}
